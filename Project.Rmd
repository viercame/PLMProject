---
title: 'Peer-graded Assignment: Prediction Assignment Writeup'
author: "Vicente Castro"
date: "18 de febrero de 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Introduction
The goal of this project is to predict the manner in which they did the exercise. I've created a report describing how I built the model, how I used cross validation, what I think the expected out of sample error is, and why I made the choices I did.


#Data preparation


```{r}
library(caret)

training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")

dim(training)
```

Reduce the number of predictors by removing variables with nearly zero variance,  NA.

```{r }

# remove variables with nearly zero variance

nzv <- nearZeroVar(training)

subTraining <- training[, - nzv]
dim(subTraining)

# remove variables that are almost always NA
mostNA <- sapply(subTraining, function(x) mean(is.na(x)))>0.9
subTraining <- subTraining[, mostNA==F]
dim(subTraining)

#remove variables that don't make intuitive sense for prdiction (V1 seems to be a serial number and cvtd_timestamp)

subTraining <- subTraining[,-1]
subTraining <- subTraining[, c(1:3, 5:58)]

dim(subTraining)

```

#Creating cross validation data

How I will demostrate at the end pof this report, the best model is Rain forrest, and according with the lectures and http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#overview, there is no need to create a separate set of validation; however, as it is one of the project evaluation criteria, there is no problem to create a cross validation dataste to compare the model created by the training subset.

```{r }

#Divide trainign set for validation

inTrain <- createDataPartition(subTraining$classe, p=0.7, list=FALSE)

subTrainingModel <- subTraining[inTrain,]
subValidation <- subTraining[-inTrain,]

dim(subTrainingModel)
dim(subValidation)

```
#Creating prediction model
Using the first set of data, to create the prediction model (using random forest).

According with some forums and other resources , first, I setup to run in parallel, using all the CPU cores available.


```{r }

model <- "modelFit.RData"
set.seed(2017)
if (!file.exists(model)){
    require(parallel)
    require(doParallel)
    cl <- makeCluster(detectCores() - 1)
    registerDoParallel(cl)
    
    fitModelRF <- train(subTrainingModel$classe ~ ., method = "rf", data = subTrainingModel)
    save(fitModelRF, file = "modelFit.RData")

    stopCluster(cl)
    
}else{
  
  load(file="modelFit.RData", verbose = TRUE)
  
  
}
```

#Measure the Accuracy and sample error



```{r }
predictTrain <- predict(fitModelRF, subTrainingModel)

confusionMatrix(predictTrain, subTrainingModel$classe)

```
Now, I'm going to use the validation subset and creat a prediction.



```{r }
predictValidation <- predict(fitModelRF, subValidation)

confusionMatrix(predictValidation, subValidation$classe)

```


From the validation subset, the accuracy is still hig, above 99.9%.

Given the high level of accuracy, I think there is no need to build another prediction model for better accuracy. These will only complicate the exercise - making it hard to explain, and takes too long a time to run another training process.

##List of important predictor in the model

```{r }
varImp(fitModelRF)


```

Final Model

```{r }


fitModelRF$finalModel
```

The reported OOB estimated error is at 0.09%, the prediction model should be applied to the final testing set, and predict th classe in the 20 test cases.

#Apply the prediction Model

```{r }

predictTesting <- predict(fitModelRF, testing)
predictTesting
```
